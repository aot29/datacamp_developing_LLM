{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3887a48b-c24c-4db8-a7d8-e495fb492ced",
   "metadata": {},
   "source": [
    "# Neural Network Architecture and Hyperparameters\n",
    "\n",
    "## Activation functions between layers\n",
    "Sigmoid and Softmax are usually applied on the last layer of a network.\n",
    "\n",
    "Sigmoid\n",
    "* Output between 0 and 1\n",
    "* Can be used anywhere in the network\n",
    "* Gradients approach zero for low and high values of x (saturation)\n",
    "* Some values are spo small, that they can prevent the gradient from updating (vanishing gradient) -> training the network is challenging\n",
    "\n",
    "Softmax\n",
    "* Output between 0 and 1\n",
    "* Saturates\n",
    "* Cannot be used anywhere\n",
    "\n",
    "### ReLu\n",
    "Rectified Linear Unit\n",
    "\n",
    "f(x) = max(x, 0)\n",
    "\n",
    "* positive input: output equal to input\n",
    "* negative input: 0\n",
    "\n",
    "* Widely used\n",
    "* No upper bounds\n",
    "* gradient does not approach 0 -> no vanishing gradient problem\n",
    "* However, once an input element is negative, it will be set to 0 for the rest of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "514fd7cc-2799-4509-86a6-cf3070b712e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a ReLU function with PyTorch\n",
    "relu_pytorch = nn.ReLU()\n",
    "\n",
    "# Apply your ReLU function on x, and calculate gradients\n",
    "x = torch.tensor(-1.0, requires_grad=True)\n",
    "y = relu_pytorch(x)\n",
    "y.backward()\n",
    "\n",
    "# Print the gradient of the ReLU function for x\n",
    "gradient = x.grad\n",
    "print(gradient) # 0, as input value is -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca940587-018e-4f7b-b3e9-f6f540381176",
   "metadata": {},
   "source": [
    "### Leaky ReLu\n",
    "\n",
    "* positive input: output equal to input\n",
    "* negative input: output is input multiplied by a small coefficient (e.g. 0.01) -> non-0 gradient for negative input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ffa27d5-656b-4162-a48e-b8742b8796f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a leaky relu function in PyTorch\n",
    "leaky_relu_pytorch = nn.LeakyReLU(negative_slope=0.05)\n",
    "\n",
    "x = torch.tensor(-2.0)\n",
    "# Call the above function on the tensor x\n",
    "output = leaky_relu_pytorch(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b0d1ea-5d10-411c-a54f-c05bc7dacaf2",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Fully connected NN\n",
    "* Linear layers are fully connected\n",
    "* A neuron computes a linear operation using N+1 parameters, where N is number of outputs from previous layer, +1 for bias\n",
    "* 3 types of layers: input, hidden, output\n",
    "* Size of input layer is number of features\n",
    "* Size of ouput layer is number of classes\n",
    "* Number of features and classes are imposed by dataset\n",
    "* \"model capacity\" = number of parameters in the model (in the hidden layers) -> solve more complex problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ef81912-f56d-4d24-b9d5-983542d2358d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example model\n",
    "model = nn.Sequential(nn.Linear(8, 4), nn.Linear(4, 2))\n",
    "# model capacity\n",
    "# 1st layer: 4 neurons, 8 inputs -> 4 * (8 + 1) = 36 parameters\n",
    "# 2nd layer: 2 neurons, 4 inputs -> 2 * (4 + 1) = 10 parameters\n",
    "\n",
    "total = 0\n",
    "for param in model.parameters():\n",
    "    total += param.numel()\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db17133-cf5c-45c5-8317-eedf95212817",
   "metadata": {},
   "source": [
    "## Learning rate and momentum\n",
    "\n",
    "Trainig a nNN amounts to solving an optimizatin problem. \n",
    "Stochastic Gradient Descent (SGD) is the usual optimizer. SGD takes 2 params:\n",
    "* learning rate: step size = gradient * learning rate (typically 0.01 to 0.0001)\n",
    "* momentum: controls inertia, enables to overcome local dips and find global minimum (typically 0.85 to 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b63793-1384-4976-ae10-8a7f3b06b60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49255b17-a96a-4159-83da-34f08de75bed",
   "metadata": {},
   "source": [
    "## Layer initialization and transfer learning\n",
    "Layers can be initialized by sampling from a uniform distribution. Initial weights are from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f780a8ff-1396-4303-b6ad-2c1de0631e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.2915e-06, grad_fn=<MinBackward1>),\n",
       " tensor(0.9997, grad_fn=<MaxBackward1>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "layer = nn.Linear(64, 128)\n",
    "nn.init.uniform_(layer.weight)\n",
    "layer.weight.min(), layer.weight.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0eaed6fd-aaa3-45ac-8475-251fc6648873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another example\n",
    "layer0 = nn.Linear(16, 32)\n",
    "layer1 = nn.Linear(32, 64)\n",
    "\n",
    "# Use uniform initialization for layer0 and layer1 weights\n",
    "nn.init.uniform_(layer0.weight)\n",
    "nn.init.uniform_(layer1.weight)\n",
    "\n",
    "model = nn.Sequential(layer0, layer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c49b09-3a19-4d78-928b-9ef2dfd19bcd",
   "metadata": {},
   "source": [
    "Fine-tuning\n",
    "> In practice, a model is typically trained on a large dataset as a starting point, and re-tuned on a smaller one, __but with a smaller learning rate__.\n",
    "> \n",
    "> Some layers can be left untrained = freezed: generally, freeze early layers and tune layers closer to the output\n",
    "\n",
    "Steps\n",
    "* Find a model trained on a similar task\n",
    "* Load weights\n",
    "* Freeze some layers if necessary\n",
    "* train with smaller learning rate\n",
    "* evaluate loss and adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df55ccad-6f58-4e5d-b7c0-9e2ac5e70ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(nn.Linear(64, 128), nn.Linear(128, 256))\n",
    "\n",
    "for name, param in model.named_parameters():    \n",
    "  \n",
    "    # Check if the parameters belong to the first layer\n",
    "    if name == '0.weight' or name == '0.bias':\n",
    "      \n",
    "        # Freeze the parameters\n",
    "        param.requires_grad = False\n",
    "  \n",
    "    # Check if the parameters belong to the second layer\n",
    "    if name == '1.weight' or name == '1.bias':\n",
    "      \n",
    "        # Freeze the parameters\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ec8ca-d217-400e-90f8-6260fc1fb727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
