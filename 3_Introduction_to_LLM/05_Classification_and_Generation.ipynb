{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae03e7b9-1861-46db-9455-4ee90eff6cf3",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "__pipelines__\n",
    "* Simple interface\n",
    "* automatic model selection\n",
    "* less control\n",
    "* less flexibility in choice of task\n",
    "\n",
    "__auto classes__\n",
    "* Flexibility, customization\n",
    "* manual setup is complex\n",
    "\n",
    "Example:\n",
    "* load pre-trained model weights and tokenizer by name\n",
    "* model_name aka \"model checkpoint\"\n",
    "\n",
    "AutoModel does not provide a head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742380cb-e1ab-4230-b8f1-8d0230766fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atroncos/anaconda3/envs/pt_llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/atroncos/anaconda3/envs/pt_llm/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "text = \"I am an example for text classification\"\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b277e-577e-4f3c-97d6-d10a575a3bd8",
   "metadata": {},
   "source": [
    "Tokenize inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4373392f-135c-4a95-ad87-eb6137be98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a2608-81b0-4700-aa07-75a73a670efb",
   "metadata": {},
   "source": [
    "get model's hidden states:\n",
    "* pooler_output: high-level aggregated\n",
    "* last_hidden_states: raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b84c6b04-8646-447d-99d2-b8ed4f399646",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)\n",
    "pooled_output = outputs.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30e67efb-9bae-4b12-ab8a-c4861e40370b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747b8959-26e9-4d58-b42f-83f4bc673f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3a4832-b35b-46c1-b5b7-537b746eeaea",
   "metadata": {},
   "source": [
    "Forward through custom classification head to obtain class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dec4dae8-c92c-4c66-87c7-1f3d378565a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4355, 0.5645]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "classifier_head = SimpleClassifier(pooled_output.size(-1), num_classes=2)\n",
    "logits = classifier_head(pooled_output)\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4e84df-e526-4dd0-807b-c7cc181f6675",
   "metadata": {},
   "source": [
    "Autoclass with preconfigured head\n",
    "\n",
    "AutoModelForSequenceClassification: sentiment classification in a 5 star rating scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2a12ee2-8438-4e8b-a8cb-707021d790fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atroncos/anaconda3/envs/pt_llm/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de0811b3-c04b-4d20-89b5-e17cc0b524f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The quality of the product was just okay.\"\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "predicted_class + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04c9d5f-695f-4bc2-b69e-96acd81ad38c",
   "metadata": {},
   "source": [
    "# Text generation\n",
    "\n",
    "* AutoModelForCausalLM accepts auto-regressive models like gpt2\n",
    "* Model head for next word prediction\n",
    "* takes prompt and generates max_length tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a5606fe-6476-4301-8501-3854dab2ff14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"This is a simple example for text generation, but it's also a good way to get a feel for how the text is generated\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"This is a simple example for text generation,\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "output = model.generate(inputs, max_length=26)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fb37eb-fed5-4d5f-8be9-f48e07ea7d5c",
   "metadata": {},
   "source": [
    "# Datasets for text classification\n",
    "Example: load imdb reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02b397dd-6215-48ad-8099-1831141157b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atroncos/anaconda3/envs/pt_llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = load_dataset('imdb')\n",
    "train_data = dataset['train']\n",
    "dataloader = DataLoader(train_data, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de502dbe-d73a-4754-bc53-7a9f96caf3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "------------------------------\n",
      "['Cute film about three lively sisters from Switzerland (often seen running about in matching outfits) who want to get their parents back together (seems mom is still carrying the torch for dad) - so they sail off to New York to stop the dad from marrying a blonde gold-digger he calls \"Precious\". Dad hasn\\'t seen his daughters in ten years, they (oddly enough) don\\'t seem to mind and think he\\'s wonderful, and meanwhile Precious seems to lead a life mainly run by her overbearing mother (Alice Brady), a woman who just wants to see to it her daughter marries a rich man. The sisters get the idea of pushing Precious into the path of a drunken Hungarian count, tricking the two gold-digging women into thinking he is one of the richest men in Europe. But a case of mistaken identity makes the girls think the count is good-looking Ray Milland, who goes along with the scheme \\'cause he has a crush on sister Kay.<br /><br />This film is enjoyable, light fare. Barbara Read as Kay comes across as sweet and pretty, Ray Milland looks oh so young and handsome here (though, unfortunately, is given little to do), Alice Brady is quite good as the scheming mother - but it is Deanna Durbin, a real charmer and cute as a button playing youngest sister Penny, who pretty much steals the show. With absolutely beautiful vocals, she sings several songs throughout the film, though I actually would have liked to have seen them feature her even more in this. The plot in this film is a bit silly, but nevertheless, I found the film to be entertaining and fun.', 'I saw Soylent Green back in 1973 when it was first released and maybe another eight times over the years on T.V. or video. It was always one of my favorite sci-fi and/or Charlton Heston films.<br /><br />Recently, the Egyptian theater in L.A. had a twelve film Charlton Heston retrospective. I flew in from out of state to see six of the films over a two day period. Soylent Green looked great on the large Egyptian screen with a perfect new print. From its opening montage to the going home scene to the great ending the film was fantastic.<br /><br />Charlton Heston as a cop who lives in a dog eat dog world with few natural resources left and no understanding as to how the world used to be and Eddie Robinson as a man who remembers the past are both great.<br /><br />Their chemistry together is wonderful. The film also looks so much better in a great 35mm print. Fleisher really knows how to fill the screen,and the cinematoraphy, writing, music used, and everything about it works. The film is also very powerful in its bleak and very possible view of the future. Just think how the world population grew, the rain forest that disappeared, resources used up, green house effect getting worse since 1973. I just wonder why this film has not played in theaters all these years. Its reputation should be better.<br /><br />Speaking of reputations, often people speak as if Charlton Heston is not a great actor. Seeing him in El-Cid, Soylent Green, The Warlord, The Omega Man, Will Penny, and Major Dundee back to back I am convinced he is one of our best actors. Of course he made about a dozen other great films and for those that care you know what they are.<br /><br />']\n",
      "------------------------------\n",
      "Label: tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# print some examples\n",
    "i = 0\n",
    "batch = next(iter(dataloader))\n",
    "print(f\"Example {i + 1}:\")\n",
    "print(\"------------------------------\")\n",
    "print(batch['text'])\n",
    "print(\"------------------------------\")\n",
    "print(\"Label:\", batch['label'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c065fb-429c-4116-907b-826b8bf2c94e",
   "metadata": {},
   "source": [
    "Load a dataset from Stanford NLP catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e0a7dc9-41f1-4b49-aed6-7d5a89c27aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: jqcxwi\n",
      "Paragrpah: How can I purposely get clumps in my spaghetti Ok this is a weird one guys, but I have an autistic kid and his absolute favourite thing in the world to eat is 'spaghetti chunk'... so like you know when you boil the dried pasta and you get a little lump where some of the spaghetti has fused together? I dont know if I'm explaining this properly but anyway it's his birthday tomorrow and I really wanna make him a bowl of 'spaghetti chunk' and meatballs for his birthday meal (as we can't go out to celebrate due to lockdown)  So yeah I know this is an odd question but how can I cook/prepare the pasta so I can give him a full bowl of chunks? I only have 2 300g packs so not enough for a load of trial and error. I was gonna snap it and cook it in as little water as possible but I really dont know if that will work. Sorry for bizarre question but my son would literally be beside himself with happiness if I were to cook him a big bowl of his goddamn chunks... Thanks in advance if anyone has any ideas lol\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"stanfordnlp/shp-2\", data_dir=\"reddit/askculinary\")\n",
    "train_data = dataset['train']\n",
    "example = train_data[1]\n",
    "print(\"Title:\", example[\"post_id\"])\n",
    "print(\"Paragrpah:\", example[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d08c317-3be2-4ea7-9d2c-5489e30e4d3b",
   "metadata": {},
   "source": [
    "### Train the LLM for next word prediction\n",
    "\n",
    "Example: \"the cat is sleeping on the mat\"\n",
    "\n",
    "An **input sequence** is a segment of text, e.g. \"the cat is\"\n",
    "\n",
    "A **target sequence** is the same as the input, but shifted 1 token to the left, e.g. \"cat is sleeping\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae025a-5542-4cbf-8e50-e3fce04d55e0",
   "metadata": {},
   "source": [
    "### Classifying movie opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc733218-e272-4b5b-a737-53f9df83d747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atroncos/anaconda3/envs/pt_llm/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for \"The best movie I've ever watched!\": 1\n",
      "Predicted class for \"What an awful movie. I regret watching it.\": 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"textattack/distilbert-base-uncased-SST-2\"\n",
    "# Load the tokenizer and pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "  model_name, num_labels=2)\n",
    "\n",
    "text = [\"The best movie I've ever watched!\", \"What an awful movie. I regret watching it.\"]\n",
    "\n",
    "# Tokenize inputs and pass them to the model for inference\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "\n",
    "predicted_classes = torch.argmax(logits, dim=1).tolist()\n",
    "for idx, predicted_class in enumerate(predicted_classes):\n",
    "    print(f\"Predicted class for \\\"{text[idx]}\\\": {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d0d0c-9efa-474c-84fc-c364c71ccf1c",
   "metadata": {},
   "source": [
    "### Text generation example\n",
    "\n",
    "Arrange the code pieces in the right order to load Stanford NLP's \"askculinary\" dataset, then extract a short sequence (first 60 characters) from the 'history' attribute of the first training instance, and finally use the extracted sequence as a prompt for the \"gpt\" model to generate follow-up text upon the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e29e53b0-76c1-41b5-93b2-a0adcd901c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "dataset = load_dataset(\"stanfordnlp/shp-2\", data_dir=\"reddit/askculinary\")\n",
    "train_data = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04b35d5f-6088-432a-b387-e88fa8c530b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How can I purposely get clumps in my spaghetti Ok this is a '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = train_data[0][\"history\"][:60]\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12917e16-2e41-47b0-a847-beca47fc5fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atroncos/anaconda3/envs/pt_llm/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ef8fba4-67ce-4c1b-80c4-1fb275a14174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "output = model.generate(inputs, max_length=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ba823a5-c7b7-44d2-a3d9-a0397f194af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How can I purposely get clumps in my spaghetti Ok this is a \\xa0problem. I have a lot of spaghetti and I have to make'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf329f46-180e-47ec-b25f-e6df60a4e20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf46ccc-022e-4579-b0c0-5d652465ea7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
