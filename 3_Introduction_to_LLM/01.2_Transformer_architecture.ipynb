{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94164b3-4696-423d-a17c-36b81894a796",
   "metadata": {},
   "source": [
    "# The transformer architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bcfbc9-c690-4219-a606-231bd7bb0c1d",
   "metadata": {},
   "source": [
    "example: LLM, BERT, T5\n",
    "\n",
    "* No RNN layers architecture\n",
    "* Better at long-range dependencies: Attention mechanisms, positional encoding\n",
    "* Parallelization\n",
    "\n",
    "Original transformer: A. Vaswani (2017) \"Attention is all you need\"\n",
    "\n",
    "* 2 main stacks: encoder and decoder\n",
    "* N encoder (resp decoder) layers\n",
    "* No recurrence or convolutions\n",
    "\n",
    "Suitable for language tasks requiring text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5ce3820-9d67-44b0-a946-192abceba436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################\n",
    "#   This is an umfinished squeleton    #\n",
    "########################################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# model embedding dimension: dimension of inputs, outputs, inbetween processing\n",
    "d_model = 512\n",
    "# number of attention heads, a divisor of d_model\n",
    "n_heads = 8\n",
    "# number of encoder and decoder layers\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "\n",
    "model = nn.Transformer(\n",
    "    d_model=d_model,\n",
    "    nhead=n_heads,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ecaaf-e7e7-43ce-aac8-77d1e726fd59",
   "metadata": {},
   "source": [
    "### Types of transformers\n",
    "\n",
    "| Type | tasks | Models |\n",
    "| ---- | ----- | ------ |\n",
    "| Encoder-decoder | Translation, summarization | T5, BART, mBART |\n",
    "| Encoder only | Classification, sentiment analysis, extractive QA | BERT, ALBERT, DistilBERT, ELECTRA, RoBERTa |\n",
    "| Decoder-only | Text generation, generative QA | GPT, ~~GPT-2~~, ~~CTRL~~ |\n",
    "\n",
    "see: \n",
    "* https://huggingface.co/learn/nlp-course/chapter1/1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceafd598-8eb1-4e68-8a57-4f04a1dcbc53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
