{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2260b5c9-d331-42af-bfd6-728b9a7f7784",
   "metadata": {},
   "source": [
    "# Building a Decoder Transformer\n",
    "* simplified version of the original transformar\n",
    "* sequence generation not requiring an encoder: text generation, text completion\n",
    "\n",
    "Particularities:\n",
    "* Masked multi-head self-attention\n",
    "* Upper triangular mask: prevent attending to future tokens, can only observe previous tokens\n",
    "* Decoder head: linear + softmax activation to predict most lkely token from the entire vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c85807-f4ff-4596-9981-82279caf2329",
   "metadata": {},
   "source": [
    "## Maked self-attention\n",
    "Key to causal behaviour. Triangular mask:\n",
    "```\n",
    "Orange   1 0 0 0 0\n",
    "is       1 1 0 0 0\n",
    "my       1 1 1 0 0\n",
    "favorite 1 1 1 1 0\n",
    "fruit    1 1 1 1 1\n",
    "```\n",
    "For example: in \"Orange in my favorite fruit\", the token \"favorite\" only pays attention to \"Orange\", \"is\", \"my\", \"favorite\" (4th row in the matrix). The model would learn that the probable next word is \"fruit\".\n",
    "\n",
    "Same multi-head attention as in encoder transformer, only the mask is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ca975-0724-4782-b93b-64d5d8b94174",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attention_mask = (1 - torch.triu(\n",
    "    torch.ones(1, sequence_length, sequence_length), diagonal=1)).bool()\n",
    "#...\n",
    "output = decoder(input_sequence, self_attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cacaeb-b3cb-44cb-85ed-9ca8bed49d08",
   "metadata": {},
   "source": [
    "## Transformer body and head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee5b9d40-86a5-4c0b-b476-77b78300b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout, max_sequence_length)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        # Add a linear layer (head) for next-word prediction\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, self_mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask)\n",
    "\n",
    "        # Apply the forward pass through the model head\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ffe2c-76ae-4ad6-8474-b4f332fe3e24",
   "metadata": {},
   "source": [
    "Test using random sequence. Output is next-token probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0881dbf-58ff-43bb-b796-d72d904e5d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "vocab_size = 10000\n",
    "batch_size = 8\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "sequence_length = 64\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d4a673b-a233-429e-bea4-1637f445e088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 64, 10000])\n",
      "tensor([[[ -8.4589, -10.2904,  -9.2712,  ..., -10.3509,  -8.9835,  -9.8259],\n",
      "         [ -8.9041, -10.2764,  -9.1243,  ...,  -9.5853,  -9.0880,  -9.2692],\n",
      "         [ -8.9674,  -9.3047,  -9.6107,  ..., -10.2786,  -8.6873,  -9.6084],\n",
      "         ...,\n",
      "         [ -9.2677,  -9.5564,  -9.5862,  ..., -10.4316, -10.1462,  -8.9992],\n",
      "         [ -8.9629, -10.0210,  -8.4622,  ...,  -9.3217,  -9.6332,  -9.2137],\n",
      "         [ -9.6159, -10.1622,  -9.7675,  ...,  -9.4639,  -9.5766, -10.7483]],\n",
      "\n",
      "        [[ -9.4188, -10.2605,  -9.3097,  ...,  -8.8197,  -9.3785,  -8.7900],\n",
      "         [ -9.4871,  -9.5820,  -8.6926,  ...,  -9.2533,  -9.1897,  -9.1286],\n",
      "         [ -9.1479,  -9.5352,  -8.5771,  ...,  -9.4810,  -8.9143, -10.1448],\n",
      "         ...,\n",
      "         [ -9.4951,  -9.8587,  -9.3739,  ...,  -9.5837,  -8.5317,  -8.6408],\n",
      "         [-10.0069, -10.0204,  -9.6759,  ...,  -9.0715,  -9.4575,  -9.5549],\n",
      "         [ -8.3765,  -9.9367,  -9.7127,  ...,  -9.6211, -10.2719,  -9.4033]],\n",
      "\n",
      "        [[ -8.6498, -10.2336,  -9.6098,  ...,  -9.2718,  -8.8139,  -9.6843],\n",
      "         [ -8.7722, -10.2357,  -9.4654,  ...,  -9.5116,  -9.4034,  -9.3544],\n",
      "         [ -9.0451,  -9.8674,  -9.3945,  ...,  -8.8301,  -9.7258,  -9.5882],\n",
      "         ...,\n",
      "         [ -9.8928, -10.7297,  -9.0934,  ...,  -9.1879,  -9.2870,  -9.4007],\n",
      "         [ -9.8682, -11.2025,  -9.5913,  ...,  -9.2924,  -8.4658,  -9.7231],\n",
      "         [ -9.0315,  -9.6688,  -9.4233,  ...,  -9.6253,  -9.0884,  -9.2556]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ -9.7168,  -9.5343,  -8.8937,  ...,  -9.5426,  -9.8096, -10.1306],\n",
      "         [ -8.8551, -11.1472,  -8.3555,  ...,  -8.8915,  -9.2199,  -9.4387],\n",
      "         [ -9.1003, -10.0808,  -9.1080,  ...,  -9.3354,  -9.0000,  -9.4591],\n",
      "         ...,\n",
      "         [ -9.4425, -11.0465,  -9.0525,  ...,  -9.5335,  -8.7633,  -9.6144],\n",
      "         [ -9.3652, -10.0085,  -8.5776,  ...,  -8.8053,  -9.2975,  -9.6017],\n",
      "         [ -7.7852, -10.5905,  -8.9510,  ...,  -9.6446,  -8.6703,  -9.1927]],\n",
      "\n",
      "        [[ -9.1293, -10.8474,  -9.1614,  ...,  -8.7826,  -8.4443,  -9.1205],\n",
      "         [ -9.1160, -10.4090,  -9.1569,  ...,  -9.5419,  -9.3844,  -9.0651],\n",
      "         [ -9.2188, -10.3532,  -9.4649,  ...,  -9.4268,  -8.6180,  -9.9041],\n",
      "         ...,\n",
      "         [ -9.5315, -10.8660,  -9.2711,  ...,  -9.0992,  -9.2905, -10.8145],\n",
      "         [ -9.0128,  -9.6539,  -8.4183,  ...,  -8.9381,  -9.1092,  -9.5767],\n",
      "         [ -9.3759, -10.1528, -10.0468,  ...,  -9.5465,  -9.4195,  -8.8302]],\n",
      "\n",
      "        [[ -8.8152,  -9.4765,  -8.7423,  ...,  -8.6042,  -8.8105, -10.2366],\n",
      "         [ -8.8899,  -9.5714,  -9.1298,  ...,  -9.1211,  -9.4051,  -9.4308],\n",
      "         [ -8.5928, -10.0532,  -9.0305,  ...,  -9.9905,  -8.9376,  -9.1485],\n",
      "         ...,\n",
      "         [ -8.3697,  -9.8504,  -9.1720,  ...,  -9.3216,  -9.0026,  -9.2159],\n",
      "         [ -9.6852, -10.5379,  -8.7196,  ...,  -9.2321,  -9.3052, -10.2076],\n",
      "         [ -8.9481,  -9.4999,  -8.5624,  ...,  -8.8825,  -9.7164,  -9.6436]]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "# Create a triangular attention mask for causal attention\n",
    "self_attention_mask = (1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)).bool()  # Upper triangular mask\n",
    "\n",
    "# Instantiate the decoder transformer\n",
    "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "output = decoder(input_sequence, self_attention_mask)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74664725-3721-4d27-9ce7-37802746631d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f2ceafa-56cc-4948-aa70-3e193616769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c987361-7126-4dfc-b354-c9901ea8dbc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ad073af-0b1e-405e-8e21-f992aa0ee034",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x+self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x+self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f801d-f7d2-40e1-999d-451f8de35773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34293fdb-af78-4a1f-97fe-2f13fb9c16dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        num_heads: number of attention heads, each handling embeddings of size head_dim\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # define 3 linear transformatins for attention input\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # define linear transformation for the final concatenated output\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the inputs accross attention heads\"\"\"\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim)\n",
    "\n",
    "    def compute_attention(self, query, key, mask=None):\n",
    "        \"\"\"Computes attention weights inside each head.\"\"\"\n",
    "        # Compute dot-product attention scores\n",
    "        scores = torch.matmul(query, key.permute(1, 2, 0))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-1e9\"))\n",
    "        # Normalize attention scores into attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        return attention_weights\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # attention weights for Q, K, V\n",
    "        query = self.split_heads(self.query_linear(query), batch_size)\n",
    "        key = self.split_heads(self.key_linear(key), batch_size)\n",
    "        value = self.split_heads(self.value_linear(value), batch_size)\n",
    "\n",
    "        # concatenate\n",
    "        attention_weights = self.compute_attention(query, key, mask)\n",
    "\n",
    "        # Multiply attention weights by values, concatenate and linearly project outputs\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        output = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.output_linear(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c8f599-72c4-4bf2-9bca-29fd0028ac63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "948f9cf5-280b-4c94-b3b8-2aa554889bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FeedForwardSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForwardSubLayer, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfcb357-c9ff-42ee-b96d-107bc1b502d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
