{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09159e7d-df94-4274-aaf8-15875aca3df4",
   "metadata": {},
   "source": [
    "# Building Encoder Decoder\n",
    "\n",
    "Extra: Cross-attention, links transformer main 2 blocks\n",
    "* Takes 2 inputs: information coming from the decoder, final hidden states from encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef061e-14fd-4ac0-914c-026abab7aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # Initialize the causal (masked) self-attention and cross-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, causal_mask, encoder_output, cross_mask):\n",
    "        # Pass the necessary arguments to the causal self-attention and cross-attention\n",
    "        self_attn_output = self.self_attn(x, x, x, causal_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))\n",
    "        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, cross_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185380d9-9b68-4f5c-8608-91008b78a8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87b323bb-cf1d-4140-8df1-f193561e063a",
   "metadata": {},
   "source": [
    "code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631a63b0-cc3c-42e4-a4a2-bee20cf310d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    ...\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    ...\n",
    "\n",
    "class FeedForwardSubLayer(nn.Module):\n",
    "    ...\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    ...\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    ...\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    ...\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    ...\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13f4515-8a37-443e-96bf-69059defa97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = TransformerEncoder(vocab_size, d_model, num_heads, num_layers, num_heads, d_ff, max_seq_len, dropout)\n",
    "        self.decoder = TransformerDecoder(vocab_size, d_model, num_heads, num_layers, num_heads, d_ff, max_seq_len, dropout)\n",
    "\n",
    "    def forward(self, src, src_mask, causal_mask):\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        decoder_output = self.decoder(src, ecoder_output, causal_mask, mask)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34925672-0e1f-4bc9-97eb-5002b7a17d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "batch_size = 16\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "sequence_length = 64\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e879a-06d7-4894-a409-b884d6f2048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of random input sequences\n",
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "padding_mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "causal_mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1)\n",
    "\n",
    "# Instantiate the two transformer bodies\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "# Pass the necessary masks as arguments to the encoder and the decoder\n",
    "encoder_output = encoder(input_sequence, padding_mask)\n",
    "decoder_output = decoder(input_sequence, causal_mask, encoder_output, padding_mask)\n",
    "print(\"Batch's output shape: \", decoder_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262ab54-9e9c-4f26-989b-a217653467e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize positional encoding layer and stack of EncoderLayer modules\n",
    "class TransformerEncoder(nn.Module):\n",
    "  \n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass the sequence through each layer in the encoder\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        # Initialize the encoder stack of the Transformer\n",
    "        self.encoder = TransformerEncoder(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        return encoder_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
